# Tree search algorithms cont'd 
- For the beginning part of tree search algorithms, refer to [[CHAPTER_4_AGENTS_AND_SEARCH_ALGORITHMS#Tree search algorithms]]
## States vs nodes 
- A **state** is a representation of a physical configuration whereas a **node** is a data structure constituting part of a search tree, this includes: 
	- Parents
	- Children
	- Depth
	- Path cost $g(x)$
- States do not have parents, children, depth or a path cost. 
![[state_representation.PNG]] 
- The **Expand** function essentially creates new nodes and fills in the various fields by using a callback sequence to the **Successor** function of the problem which then will create the corresponding states for the newly created nodes. 
```javascript 
function tree_search(problem, fringe) returns a solution or failure
	 fringe = insert(make_node(initial_state[problem]), fringe)
	 loop do 
		 if fringe.is_empty() then return failure
		 node = remove_front(fringe)
		 if goal_test(problem, state(node)) then return node
		 fringe = insert_all(expand(node, problem), fringe)
```

```javascript 
function Expand(node, problem) returns a set of nodes
	successors = the empty set
	for each action, result in successor_function(problem, state[node]) do
		s = new node 
		parent_node[s] = node
		action[s] = action 
		state[s] = result 
		path_cose[s] = path_cost[node] + step_cost(node, action, s)
		depth[s] = depth[node] + 1
		successors.add(s)
	return successors
```
### Best-First Search 
 - A general approach to picking which node from the frontier to expand next is called **Best-First Search**. This method consists of choosing a node with a minimum value given by some evaluation function $f(n)$. 
 - On each iteration, we chose a new node on the frontier with minimum $f(n)$ value, we return it if its state is a goal state otherwise, we apply $Expand()$ to generate more child nodes. 
 - Each child node is added to the frontier if it hasn't been reached before, or, it's re-added if its now being reached **with a path that has a lower path cost** than any previous path. 
 - The algorithm returns either an indication of failure, or a node that represents a path to a goal. 
### Search Data Structures
- Search algorithms generally require a data structure to keep track of the search tree. 
  A node in the tree is represented by a data structure with **four components**: 
	-  `node.state`: The corresponding state of the node. 
	- `node.parent`: The node in the tree that generated this node. 
	- `node.action`: The action that was applied to the parents state to generate this node. 
	- `node.path_cost`: The total cost of the path from the initial state to this node. 
	  `g(node)` is used for the `path_cost`. 
- Following the **parent** pointers back from a node allows us to recover the states and actions along the path to that node. Doing this from a goal node gives us the desired solution. 
- The appropriate data structure to represent and store the frontier is a queue, due to the operations on a frontier being: 
	- `is_empty(frontier)`: Returns true only if there are no nodes in the frontier. 
	- `pop(frontier)`: Removes the top node from the frontier and returns it. 
	- `top(frontier)`: Returns the top node from the frontier. 
	- `add(node,frontier)`: Inserts a node into its proper place in the queue. 
- There are three kinds of queues used in search algorithms: 
	- Priority queues: PQ's pop the nodes with the minimum cost first according to some evaluation function $f(n)$, this is used in Best-First Search. 
	- FIFO queues: regular queue, used in BFS. 
	- LIFO stack: regular stack, used in DFS. 
- All reached states can be stored in a lookup table, where each key is a state and each value is the node for that state. 
### Redundant paths
- The search tree shown in [[CHAPTER_4_AGENTS_AND_SEARCH_ALGORITHMS#Tree search algorithms]] includes a path from **Arad to Sibiu and back to Arad.** 
- We can say that Arad is a **repeated state** in the search tree, generated by a **cycle** (Also known as a **loopy path**). So, even though the state space only has 20 states, the complete search tree is infinite due to there being no limit to how often one can traverse a loop. 
- A cycle is a special case of redundant paths. 
- Ex: We can get to Sibiu via the path Arad-Sibiu (140 miles long) or the path Arad-Zerind-Oradea-Sibiu (297 miles long). 
  The second path is redundant, it's just a worse way to get to the same state.
- Removing redundant paths allows us to complete the search for the **optimal path** a lot faster.
- There are three main approaches to eliminating redundant paths: 
	- Remember all previously reached states. 
	- Do not worry about repeating the past, it will not happen (Joke? Idk? Good one teach â˜ )
	- Compromise and only check for cycles, but not for redundant paths in general. 
- We can remember all previously reached states, allowing us to detect all redundant paths and we can keep only the best path to each state. This approach is appropriate for state spaces where there are many redundant paths, in fact, it's the preferred choice when the table of reached states will **fit in memory**. 
#### Redundant paths : Do not worry about repeating the past, it won't happen 
- There exist some problems formulations where it's very rare or simply impossible for two paths to reach the same state. 
- Ex: An assembly problem where each action adds a part to an evolving assemblage, there is an ordering of parts so that it's possible to add A and then B, but not B and then A. 
- For those problems, we call a search algorithm a **graph search** if it checks for redundant paths and a **tree-like search** if it does not. 
##### Redundant paths : Check for cycles only 
- Each node has a chain of pointers, we can check for cycles without any additional memory requirements. This is done by **following up the chain of parents** to see if the state at the end of the path has appeared earlier in the path. 
- Some implementations follow this chain all the way up, and thus eliminate all possible cycles. 
- Other implementations follow only a few key links (e.g. To the parent, grandparent and great-grandparent), thus, this would only take a constant amount of time, while eliminating all short cycles (Relying on other mechanisms to deal with the longer cycles). 
### Measuring problem-solving performance
- There are four main ways to measure an algorithms problem solving performance: 
	- Completeness: Is the algorithm guaranteed to find a solution when there is one, and to correctly report a failure when there is not? 
	- Cost optimality: Does it find a solution with the lowest path cost out of all other solutions? 
	- Time complexity: How long does it take to find a solution? This metric can be measured in seconds, or more abstractly by the number of states and actions that have been considered. 
	- Space complexity: How much memory is required to perform the search? 
### Uninformed search strategies 
- An uninformed search algorithm is given no clue about how close a state is to the goals. 
- An uninformed agent with no knowledge of Romanian geography has no clue whether going to Zerind or Sibiu is a better first step. 
- In contrast, an informed agent who knows the location of each city knows that Sibiu is much closer to Bucharest and thus more likely to be on the shortest path. 
- Uninformed strategies only make use of the information available in the problem definition, the following are all the strategies that will be considered: 
	- Breadth-First Search 
	- Uniform-Cost Search 
	- Depth-First Search 
	- Depth-Limited Search 
	- Iterative Deepening Search 
#### Breadth-First Search 
![[bfs_1.PNG]]
![[bfs_2.PNG]]
![[bfs_3.PNG]]
##### Properties of BFS 
**Complete**: Yes (if $b$ is finite)
**Time:** $1 + b + b^2 + b^3 + ... + b^d + b(b^d - 1) = O(b^{d+1})$
**Space:** $O(b^{d+1})$ - Keeps every node in memory. 
**Optimal:** Yes (if cost = 1 per step), not optimal in general. 
- In this problem, **Space** is the biggest issue, it can easily generate nodes at 100MB/s, so for 24 hours, this would total 8460GB. 
- Time and space complexities are measured in terms of: 
	- $b$ = Maximum branching factor of the search tree.
	- $d$ = depth of the least cost solution.
	- $m$ = Maximum depth of the state space (may be $\infty$)

- BFS is mainly used when all allocations have the same cost, a FIFO queue will be faster than a PQ and will yield correct node order. New nodes go to the back of the queue and old nodes get expanded first. 
- **Reached** can be a set of states rather than a mapping from states to nodes. We can also perform an **early goal** test to check whether a node is a solution as soon as it's generated, rather than the **late goal** test that Best-First Search uses. 
- BFS always finds a solution with a **minimal** number of actions, when it generates nodes at depth $d$, it has already generated all nodes at depth $d - 1$, so, if one of them were a solution, it would have already been found. This essentially means that BFS is **cost optimal** for problems where **all actions have the same cost** but not for problems that don't have that property. 

- In general, exponential problems are very scary $O(b^d)$
- A typical real-world example would be to consider a branching factor $b$ with a processing speed of 1 million nodes/s, and a memory requirement of 1 Kilobyte/node. A search to depth $d = 10$ would take less than 3 hours, but would require 10 Terabytes of memory. Memory requirements are generally a bigger problem for BFS than the execution time of the search. In general,  **exponential-complexity search problems** cannot be solved by uninformed searches for any but the smallest instances. 
##### Dijkstra's algorithm or Uniform-Cost Search
![[dijkstars_1.PNG]]
- When actions have **different costs**, an obvious choice is to use the Best-First Search where the evaluation function is the cost of the path from the root to the current node. The main idea is that while BFS spreads out in waves of uniform depth, uniform cost search **spreads out in waves of uniform path-cost**. 
- The algorithm can be implemented as a call to Best-First Search, with Path-Cost as the evaluation function.
![[dk_5.PNG]] 
![[dijkstras_2.PNG]]
![[dijkstars_3.PNG]]
![[dijkstars_4.PNG]]
![[dk_5.PNG]]
![[dk_6.PNG]]
- Uniform-Cost search is complete at this point AND is cost-optimal. This is because the first solution it finds will have a cost that is at least as low as the cost of any other node in the frontier. Uniform-Cost search considers all paths systematically in order of increasing cost and never gets caught up going down a single **infinite** path. 
#### Depth-First Search 
![[dfs_1.PNG]]
![[dfs_2.PNG]]
![[dfs_3.PNG]]
![[dfs_4.PNG]]
![[dfs_5.PNG]]
- ***after a few iterations... ***
![[dfs_final.PNG]]
- DFS is usually implemented as a tree-like search that does not a keep a table of reached states. DFS is **not cost-optimal**, it returns the first solution it finds even if it isn't cheapest. For finite spaces that are trees, DFS is efficient and complete. 
- As for acyclic state spaces, it may end up expanding the same state many times via different paths but will systematically explore the entire space. In cyclic state spaces it can sometimes get stuck in an infinite loop, therefore, some implementations of DFS check each new node for cycles. 
- For finite state spaces, DFS is not systematic, it **can get stuck going down an infinite path**, even if there are no cycles, we can therefore conclude that DFS is incomplete. 
- Why use DFS: 
	- Whenever tree-search is feasible, DFS has a much smaller memory requirement. 
	- A reached table is not necessary and the frontier is very small as well. 
- For a **finite tree-shaped** state-space, a DFS tree-like search: 
	- Takes time proportional to the number of states $O(b^m)$. 
	- Has a memory complexity of only $O(bm)$.
	  Where $b$ is the branching factor and $m$ is the maximum depth of the tree. 
##### Properties of Depth-First Search
- **Complete:** 
	- No: fails in infinite-depth spaces, spaces with loops. Needs to be modified to avoid repeated states along paths. 
	- Complete in finite spaces. 
- **Time:** $O(b^m)$ : This is terrible if $m$ is much larger than $d$. however, if solutions are dense, it may be much faster than BFS. 
- **Space:** $O(bm)$ : Linear space. 
- **Optimal:** No. 
#### Depth-Limited Search 
- Depth-Limited Search is essentially DFS with a depth limit $l$. 
- Recursive DLS pseudocode.: 
```javascript 
function depth_limited_search(problem, limit) return solution or fail or cutoff 
	recursive_dls(make_node(initial_state[problem]), problem, limit)
	
function recursive_dls(node, problem, limit) return solution or fail or cutoff
	cutoff_occurred = false
	if goal_test(problem, state[node]) then return node
	else if depth[node] = limit then return cutoff
	else for each successor in Expand(node, problem) do 
		result = recursive_dls(successor, problem, limit)
		if result = cutoff then cutoff_occurred = true
		else if result != failure then return result 
	if cutoff_occurred then return cutoff else return failure  
```
- **Idea:** To keep DFS from wandering down infinite paths, we use DLS, for which we supply a depth limit $l$ and treat all nodes at depth $l$ as if they had no successors. 
- **Time complexity:** $O(b^l)$
- **Space complexity:** $O(bl)$ 
- If we make a poor choice for $l$, the algorithm will fail to reach the solution, making it incomplete once again. 
- Since DFS is a tree-like search, we can't keep it from wasting time on redundant paths in general. We can however eliminate cycles at the cost of some computation time, if we only look a few links up the parent chain, we can catch most cycles. Longer cycles will be handled by the  depth limit. 
- Sometimes a good depth limit can be chosen based on the knowledge of the problem. 
  Ex: The map of Romania has 20 cities, therefore a valid depth limit $l$ would be $l = 19$. 
- If an extensive study of the map had been performed prior, we would have discovered that any city can be reached from any other city in at most 9 cities. 
- The specific number for the depth limit is known as the **diameter** of the state-space graph. However, for more challenging problems, we cannot know a good depth limit until we have solved the problem once before. 
##### Iterative deepening search
- Iterative deepening search solves the problem of picking a good value for $l$ by trying all possible values of $l$, that is, $l = 1, l = 2, l = 3, ...$ until either a solution is found or the DLS returns the failure value rather than the cutoff value. 
- Iterative deepening search combines many of the benefits of DFS and DLS. Like DFS, it's memory requirements are modest when there is a solution or for finite state spaces with no solution. 
- Iterative deepening search is optimal for problems where all actions have the same cost and is complete on finite acyclic state spaces or on any finite state space when we check nodes for cycles all the way up the path. 
```javascript 
function iterative_deepening_search(problem) return a solution 
	for depth = 0 to INT_MAX do 
		result = depth_limited_search(problem, depth)
		if result != cutoff then return result 
	end 
```
![[ids_1.PNG]]
![[ids_2.PNG]]
![[ids_3.PNG]]
![[ids_4.PNG]]
- Iterative deepening search may seem wasteful because states near the top of the search tree are re-generated multiple times, however, for many state spaces, most of the nodes are in the bottom level, so it doesn't matter much that the upper levels are repeated. 
- The nodes on the bottom level `depth(d)` are generated once, those on the next-to-bottom level are generated twice, and so on, up to the children of the root, which are generated $d$ times. 
- The total number of nodes generated in the worse case is:  $N(IDS) = \space (d)b^1 + (d - 1)b^2 + (d - 2)b^3 ... + b^d \rightarrow O(b^d)$ 
- For example, if $b = 10$ and $d = 5$: 
  $N(IDS) = 50 + 400 + 3000 + 20000 + 100000 = 123450 \space nodes$
  $N(BFS) = 10 + 100 + 1000 + 10000 + 100000  = 111110 \space nodes$ 
- If repetition is a concern, a hybrid approach can be used where BFS is run until almost all the available memory is consumed, it then switches to IDS from all the nodes in the frontier. 
- In general, IDS is the preferred uninformed search method when the search state space is larger than can fit in memory and the depth of the solution is not known. 
###### Properties of Iterative Deepening Search
- **Complete:** Yes. 
- **Time:** $(d+1)b^0 + db^1 + (d-1)b^2 + ... + b^d = O(b^d)$ : Exponential time. 
- **Space:** $O(bd)$ : Linear space. 
- **Optimal:** Yes, if the step cost = 1. It can also be modified to explore uniform-cost tree. 
#### Bidirectional Search 
- The algorithms covered so far all start at an initial state and can reach **any** one of **multiple possible goal states**. 
- An alternative approach, called bidirectional search, simultaneously searches forward from the initial state and backwards from the goal states. 
- In the hopes that the two searches will meet, the motivation is that: $b^{\frac{d}{2}} + b^{\frac{d}{2}}$ is much smaller than $b^d$
- For this to work some conditions must be fulfilled: 
	- We must keep track of two frontiers. 
	- We must keep track of two tables of reached states. 
	- We need to be able to reason backwards. 
- If state $s'$ is a successor of $s$ in the forward direction, then we need to know that $s$ is a successor of $s'$ in the backward direction. Thus, we will have a solution only when the two frontiers collide. 
# Summary of Uninformed Search algorithms
![[USA_summary.PNG]]
![[redundancy_img.PNG]]
