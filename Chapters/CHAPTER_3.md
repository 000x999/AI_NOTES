# Forms of learning 
- There are three main forms of agentic learning. All three types have different types of feedback that accompany the different kinds of inputs. 
	- Supervised Learning: The agent will observe input-output pairs and subsequently learn a function that maps from input to output. 
	- Unsupervised Learning: The agent will learn to discern and recognize patterns in the input data without any explicit feedback. Unsupervised Learning routines typically use clustering. 
	- Reinforcement Learning: The agent will learn from a series of reinforcement routines, typically done through a penalty/reward system. 
# Supervised Learning
- The training set of example inputs and outputs $(N)$
  - $(x_1,y_1),(x_2,y_2), \space...\space(x_N, y_N)$, Generated by an **unknown** function $f$ ,where $y = f(x)$
- The function $h$ is the hypothesis about the agent's world. This function approximates the true function $f$.
  - Typically drawn from a hypothesis space $H$ of possible functions. 
  - $h$ Model of the data, drawn from a model class $H$.
- Consistent hypothesis is a hypothesis where there is an $h$ such that each $x_i$ in the training set has $h(x_i) = y_i$
  - This looks for a **best-fit function** for which each $h(x_i)$ is **close enough to** $y_i$ 
- The true measure of a hypothesis depends on how well it handles inputs it has not yet seen or been trained on.  We say "$h$ generalizes well" if it accurately predicts the outputs of the test set.
## Hypothesis space
- The function is called a hypothesis about the world. It is typically drawn from a hypothesis space of possible functions. Ex: A set of 3rd degree polynomials, Java script functions or any other **set** of functions. 
- Alternatively, we can say that  $h$ is a model of the data drawn from a model class $H$. 
- We also typically call $y_i$, the output of the original function, the **ground-truth** as its the answer we are asking our model to predict.
- In some cases, we might have prior knowledge about the process that generated the data set. If not, we can perform **exploratory data analysis** to examine the data with various statistical tests and visualization methods such as:
	- Histograms. 
	- Scatter plots. 
	- Trying multiple hypothesis spaces and evaluating which one performs the best.
![[supervised_learning_plots.png]]
- **Bias** is also often used to analyze a hypothesis space. 
	- The tendency of a predictive hypothesis to deviate from the expected value when averaged over different training sets. Such biases often result from restrictions imposed by the hypothesis space itself. 
- **Underfitting:** Is when the model fails to find any type of pattern in the data. 
- **Variance:** Is when there is any amount of change in the hypothesis, this is typically a result due to fluctuations in the training data. 
- **Overfitting:** Is when the model pays too much attention to a particular data set it's being trained on. This causes the model to perform poorly on unseen data or other data sets, hence being **overfitted** to one specific type of data group. 
- **Bias-variance tradeoff:** Is a choice between more complex, low-bias hypotheses that fit the training data better and simpler and low-variance hypothesis that may generalize better. 
## Probabilities
- It is required to determine how **probable** a hypothesis of occurring is, not just if it's possible or impossible. 
- A hypothesis $h^*$ that is most probably given the data is: $h^* = argmax \space P(h|data), \space h \in H$
- By Bayes' rule, this is equivalent to: $h^* = argmax \space P(h|data)P(h)$. This means we must allow unusual looking functions when the data says we really need them. However, we must discourage them by previously assigning them a lower probability given the training data set.  
### A closer look at $h^* = argmax \space P(h|data)P(h)$
- $h^*$ represents the hypothesis that maximizes the expression. $P(h|data)$ represents the posterior probability of the hypothesis $h$ according to the training data. $P(h)$ is the prior probability of the hypothesis $h$. The main goal of this formula is to find the hypothesis $h^*$ that maximizes the joint probability of the posterior and prior probabilities. The $argmax$ operations denotes the argument that maximizes the expression. 
- This formula is most often used to select the most probable hypothesis according to the observed data. This is achieved by taking into consideration the prior and posterior probabilities as well as their output information.
- Calculations for these probabilities often involves making assumptions about the probability distribution. 
# Decision trees 
- Taking a look at agentic structures would be useful for this part [[CHAPTER_2#Agentic architecture and types]]
- A decision tree is a representation of a function that maps a vector of attribute values to a single output value. 
- Decision trees reach their decisions by sequentially performing tests starting at the root node and following the appropriate branch until a leaf node is reached. 
- Each internal node of the tree corresponds to a test value of one of the input attributes. 
- The branches from the nodes are labeled with all the possible attribute values.
- The leaf nodes specify what value must be returned by the function. 
- A Boolean decision tree is equivalent to a logical statement of the form: $Output\leftrightarrow (Path_1 \lor Path_2 \lor ...)$
![[decision_tree.png]]
```javascript
function LEARN_DECISION_TREE(examples, attributes, parent_examples) returns a tree
	if examples is empty then return PLURALITY_VALUE(parent_examples)
	else if all examples have the same class then return the class
	else if attributes is empty then return PLURALITY_VALUE(examples)
	else 
		A = argmax(attributes(a)) IMPORTANCE(a, examples)
		tree = a new decision tree with root test A
		for each value v of A do 
		exs = {e: examples(e) and e.A = v}	
		subtree = LEARN_DECISION_TREE(exs,attributes(A), examples)
		add branch to tree with label (A = v) and subtree subtree
	return tree
```
- The above is a decision tree learning algorithm. 
- The  ```PLURALITY_VALUE``` function selects the most common output value among a set of examples, breaking ties randomly. 
- The aim of this algorithm is to find a small tree consistent with the training examples. It achieves this by recursively choosing the **"most significant"** attribute as the root of the tree or subtree.
### Choosing attribute tests
- **Entropy:** Entropy is the measure of uncertainty of a random variable.
	- The more information the less entropy. This concept is a fundamental quantity of information theory. 
- A coin that always comes up as heads has no uncertainty and thus its entropy is defined as zero. 
- A fair coin is equally likely to come up heads or tails when flipped. The uncertainty here is $50\%$ Two possibilities with equal probabilities of $0.5$. These possibilities can be represented using a single bit. 
- A fair 4 sided die has 4 possibilities with equal probabilities. We would need 2 bits to represent these 4 possibilities. 
- Consider an unfair coin that comes up heads $99\%$ of the time. Intuitively, this coin has less uncertainty than the fair coin. If we guess heads, we'd only be wrong $1\%$ of the time. 

- In general, the entropy of a random variable $V$ with values $v_k$ having probability $P(v_k)$ is defined as: $H(V) = \Sigma_k\space P(v_k) \space log_2 (\frac{1}{P(v_k)}) = - \Sigma_k\space P(v_k) \space log_2(P(v_k))$
- Fair coin: $H(Fair) = -(0.5 \space log_2(0.5) + 0.5 \space log_2(0.5)) = 1$
- Fair 4 sided die: $H(Die4) = - 2*(0.25 \space log_2(0.25) + 0.25 \space log_2(0.25)) = 2$
- Now let's consider a loaded coin that gives heads $99\%$ of the time $H(Loaded) = -(0.99 + \space log_2(0.99) + 0.01 \space log_2(0.01) \approx 0.08 \space bits$ 
  Notice that we **ignored the second term**. 
- To better understand this, we can define $B(q)$ as the entropy of a Boolean random variable **that is true** with probability $q$. 
	- $B(q) = -(q\space log_2( q + (1 - q)\space log_2(1 - q))$
	  Thus, $H(Loaded) = B(0.99) \approx 0.08$
- The entropy of an output variable on an entire data set is:  $H(Output) = B(\frac{p}{p+n})$

- Our objective now is to choose the attribute that will maximally reduce the uncertainty (overall entropy).
- We will use that attribute as the root node of our decision tree. To do this, we take an attribute $A$ with $d$ distinct values that divide the training set $E$ into subsets $E_1, \space ... E_d$
- Each of these $d$ subsets $E_k$ has some positive $p_k$ examples and some negative $n_k$ ones. 
- So, for that specific branch, we would need: $B(p_k / (p_k + n_k))$ bits. 
- A randomly chosen example from a training set  has the $Kth$ value for the attribute $E_k$ with the probability: $(p_k + n_k)/(p + n)$
- The expected entropy remaining after testing out the attribute is: $Remainder(A) = \Sigma_d^{k=1} \space \frac{p_k + n+k}{p+n} \space B(\frac{p+k}{p_k + n_k})$
- The information gained from the attribute test on $A$ is the expected reduction in entropy: $Gain(A) = B(\frac{p}{p + n}) - \space Remainder(A)$
- Using all of this we can calculate the importance of an attribute. 
# Pruning 
- Ideally, we want our learning algorithms to find a hypothesis that fits the training data as closely as possible. More importantly, we want it to generalize well for unseen data. To do this, we often turn to something called **Decision tree pruning**. 
- Decision tree pruning helps combat model overfitting, It does this by: 
	- Eliminating nodes that are not relevant. 
	- Eliminating nodes that probably cause noise in the training data set.
- Decision tree pruning generally works by starting with a fully grown decision tree and iteratively removing branches that **do not** improve the accuracy of tree on a validation data set. 
- It iteratively removes branches and evaluates the impact of the removal on the validation set until a stopping criterion is met. The stopping criterion is often based on variety of factors such as: 
	- The size of the tree. 
	- The number of nodes removed. 
- Once the stopping criterion is met, the pruned decision tree is used to make predictions on a new data set. 